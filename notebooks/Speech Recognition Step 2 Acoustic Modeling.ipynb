{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Overview of Speech Recognition\n",
    "## 1.1 Hidden Markov Model for Accoustic Modeling\n",
    "As speech is \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Hidden Markov Model for Accoustic Modeling\n",
    "## Phoneme model of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Gaussian Mixture Model\n",
    "## Overview\n",
    "\n",
    "\n",
    "+ **Mixture of Gaussians**\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_{k}N(x|\\mu_{k}, \\Sigma_{k})$$\n",
    "> + Add a hidden variable Z, which is like a swith to each Gaussian\n",
    "  $$p(Z) = \\prod_{k=1}^{K} \\pi_{k}^{z_{k}}$$\n",
    "  + each Gaussian can be expressed as x depends on $z_{k}$\n",
    "  $$p(x | z_{k} = 1) = N(x|\\mu_{k}, \\Sigma_{k})$$\n",
    "  and\n",
    "  $$p(x | Z) = \\prod_{k=1}^{K} N(x|\\mu_{k}, \\Sigma_{k})^{z_{k}}$$\n",
    "  + Marginal Distribution of x\n",
    "  $$p(x) = \\sum_{z} p(x|z) p(z) = \\sum_{k=1}^{K} \\pi_{k} N(x|\\mu_{k}, \\Sigma_{k})$$\n",
    "  + Responsity of a Gaussion\n",
    "  $$\\begin{aligned}\\gamma(z_{k}) \\equiv p(z_{k}=1|x) = \\frac{p(z_{k}=1)p(x|z_{k}=1)}{\\sum_{j=1}^{K} p(z_{j}=1)p(x|z_{j}=1)} \n",
    "     = \\frac{\\pi_{k}N(x|\\mu_{k}, \\Sigma_{k})}{\\sum_{j=1}^{K} \\pi_{j}N(x|\\mu_{j}, \\Sigma_{j})} \\end{aligned}$$\n",
    "  \n",
    "\n",
    "+ **EM**\n",
    "> + likelihood function\n",
    "    $$ p(X|\\pi, \\mu, \\Sigma) = \\prod_{n=1}^{N} p(x_{n}|\\pi,\\mu,\\Sigma) \n",
    "    = \\prod_{n=1}^{N}\\sum_{k=1}^{K}\\pi_{k}N(x_{n}|\\mu_{k}, \\Sigma_{k}) $$\n",
    "    Take log and get log-likelihood\n",
    "    $$\\ln p(X|\\pi, \\mu, \\Sigma) = \\sum_{n=1}^{N} \\ln \\bigg\\{ \\sum_{k=1}^{K}\\pi_{k}N(x_{n}|\\mu_{k}, \\Sigma_{k}) \\bigg\\} $$\n",
    "  + set Derivative of the log-likelihood w.r.t $\\pi$, $\\mu$, $\\Sigma$ to ***Zero*** <br/>\n",
    "    Since there is a constraint with $\\pi$: $\\sum_{\\pi_{k}}^{K} = 1$, we should use ***Langrange multiplier*** method.\n",
    "    >> + Derivative of log-likelihood w.r.t $\\mu_{k}$\n",
    "    $$N(x|\\mu, \\Sigma) = \\frac{1}{(2\\pi)^\\frac{D}{2}|\\Sigma|^{\\frac{1}{2}}} \n",
    "    exp\\bigg\\{ -\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)  \\bigg\\}$$\n",
    "    $$\\nabla_{\\mu_{k}} \\ln p(X|\\pi, \\mu, \\Sigma) = \n",
    "    \\sum_{n=1}^{N} \\frac{\\pi_{k}N(x_{n}|\\mu_{k}, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_{j}N(x_{n}|\\mu_{j}, \\Sigma_{n})}\n",
    "    \\Sigma_{k}^{-1} (x_{n} - \\mu_{k}) = 0 $$    \n",
    "    $$\\mu_{k} = \\frac{1}{N_{k}} \\sum_{n=1}^{N} \\gamma (z_{nk}) x_{n}$$\n",
    "    where $N_{k} = \\sum_{n=1}{N} \\gamma (z_{nk}) $\n",
    "    + Derivative w.r.t $\\Sigma_{k}$   \n",
    "    $$\\Sigma_{k} = \\frac{1}{N_{k}} \\sum_{n=1}^{N} \\gamma(z_{nk}) (x_n-\\mu_k)(x_n-\\mu_k)^T$$\n",
    "\n",
    "    \n",
    "    \n",
    "         \n",
    "    \n",
    "  \n",
    "\n",
    "+ **VEM**\n",
    "> + Variational Method\n",
    "\n",
    "+ **Choose of Covariance** \n",
    "> + Diagonal\n",
    "> + Full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "def Gauss(x, mu, Sigma):\n",
    "    \"\"\"Multivariate diagnal covariance Gaussian\"\"\"\n",
    "    return np.exp(-np.dot(np.dot(x-mu, la.inv(Sigma)), x-mu)/2) / (np.power(2 * np.pi, len(mu)/ 2) * np.sqrt(la.det(Sigma)))\n",
    "\n",
    "def square_dot(c):\n",
    "    \"\"\" a * a' \"\"\"\n",
    "    return np.dot(c[:,np.newaxis], c[:,np.newaxis].T)\n",
    "class GMM:\n",
    "    def __init__(self,Pis_init, Mus_init, Sigmas_init, iters):\n",
    "        assert len(Pis_init) == len(Mus_init) == len(Sigmas_init)\n",
    "        self.K = len(Pis_init)\n",
    "        self.Pis = Pis_init\n",
    "        self.Mus = Mus_init\n",
    "        self.Sigmas = Sigmas_init\n",
    "        self.iters = iters\n",
    "        \n",
    "    def Gausses(self, x):\n",
    "        #return np.array([self.Pis[i] * Gauss(x, self.Mus[i], self.Sigmas[i]) for i in range(self.K)])\n",
    "        return np.array([pi * Gauss(x, mu, Sigma) for (pi, mu, Sigma) in zip(self.Pis, self.Mus, self.Sigmas)])\n",
    "    \n",
    "    def E(self, x):      \n",
    "        g = self.Gausses(x)\n",
    "        return g/np.sum(g)\n",
    "        \n",
    "    def EM(self, X):\n",
    "        N = len(X)\n",
    "        # E step\n",
    "        R_nk = np.array([self.E(x) for x in X])\n",
    "        \n",
    "        # M step\n",
    "        N_k  = np.sum(R_nk, axis=0)\n",
    "        self.Pis = N_k/N\n",
    "        # old_Mus = self.Mus\n",
    "        self.Mus = [np.sum([R_nk[n, k] * X[n] for n in range(N)], axis=0)/N_k[k] for k in range(self.K)]\n",
    "        Mus = self.Mus\n",
    "        self.Sigmas = [np.sum([R_nk[n,k] * square_dot(X[n]-Mus[k]) for n in range(N)], axis=0) /N_k[k] for k in range(self.K)]\n",
    "    \n",
    "    def params(self):\n",
    "        return [self.Pis, self.Mus, self.Sigmas]\n",
    "    def train(self, X):\n",
    "        for i in range(self.iters):\n",
    "            self.EM(X)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pis = [.3, 0.7]\n",
    "Mus = [np.array([0, 0]), np.array([1,0])]\n",
    "Sigmas = [np.array([[1,0], [0,1]]), np.array([[1,0], [0,1]])]\n",
    "x = np.array([[2,0], [0,0.5], [1,1]])\n",
    "\n",
    "gmm = GMM(Pis, Mus, Sigmas, 4)\n",
    "\n",
    "gmm.train(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2. ,  0. ],\n",
       "       [ 0. ,  0.5],\n",
       "       [ 1. ,  1. ]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.66609022,  0.33390978]),\n",
       " [array([ 0.4995783 ,  0.74978915]),\n",
       "  array([  1.99825168e+00,   1.71537899e-03])],\n",
       " [array([[ 0.24999982,  0.12499991],\n",
       "         [ 0.12499991,  0.06249996]]), array([[ 0.00178919, -0.00172336],\n",
       "         [-0.00172336,  0.00170695]])]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.dot((x[0]-Mus[0])[:, np.newaxis], (x[0]-Mus[0])[:, np.newaxis].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  0.],\n",
       "       [ 4.,  0.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([a,a], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GMM in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from sklearn import mixture\n",
    "n_comp = 64\n",
    "gmm = mixture.GMM(n_components=n_comp, covariance_type='diag')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([1,2,3]) / (np.array([1,2,3]))).dot(np.array([1,2,3]))\n",
    "np.prod(np.array([1,2,3]))\n",
    "np.dot(np.array([1,2,3]).T, np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Hidden Markov Model\n",
    "## 3.1 Markov Chains\n",
    "+ Markov Chains property\n",
    "> The current value is only dependent on the recent one\n",
    "$$p(x_n | x_1,x_2,...,x_{n-1}) = p(x_n|x_{n-1})$$\n",
    "$$p(x_1,x_2, ...,x_N) = p(x_1)\\prod_{n=2}^{N}p(x_n|x_{n-1})$$\n",
    "\n",
    "## 3.2 Hidden States\n",
    "\n",
    "\n",
    "$$ $$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
